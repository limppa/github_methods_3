---
title: "practical_exercise_5, Methods 3, 2021, autumn semester"
author: 'Linus Backström'
date: "27.10.2021"
output: pdf_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/github_methods_3/week_05")
library(tidyverse)
library(lme4)
```

# Exercises and objectives
The objectives of the exercises of this assignment are based on: https://doi.org/10.1016/j.concog.2019.03.007  
  
4) Download and organise the data from experiment 1  
5) Use log-likelihood ratio tests to evaluate logistic regression models  
6) Test linear hypotheses  
7) Estimate psychometric functions for the Perceptual Awareness Scale and evaluate them  

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__)  
REMEMBER: This is part 2 of Assignment 2 and will be part of your final portfolio


# EXERCISE 4 - Download and organise the data from experiment 1

Go to https://osf.io/ecxsj/files/ and download the files associated with Experiment 1 (there should be 29).  
The data is associated with Experiment 1 of the article at the following DOI https://doi.org/10.1016/j.concog.2019.03.007  
  
1) Put the data from all subjects into a single data frame - note that some of the subjects do not have the _seed_ variable. For these subjects, add this variable and make in _NA_ for all observations. (The _seed_ variable will not be part of the analysis and is not an experimental variable)

```{r}
df <-
    list.files(pattern = "*.csv") %>% 
    map_df(~read_csv(.))
```

    i. Factorise the variables that need factorising  
    
```{r}
df$trial <- as.numeric(df$trial)
df$trial.type <- as.factor(df$trial.type)
df$pas <- as.factor(df$pas)
df$target.contrast <- as.numeric(df$target.contrast)
df$cue <- as.factor(df$cue)
df$task <- as.factor(df$task)
df$target.type <- as.factor(df$target.type)
df$rt.subj <- as.numeric(df$rt.subj)
df$rt.obj <- as.numeric(df$rt.obj)
df$obj.resp <- as.factor(df$obj.resp)
#Anonymize subjects (also facilitates later analysis if the subjects are represented only by numbers)
df$subject <- as.numeric(df$subject)
df$subject <- as.factor(df$subject)
```

    ii. Remove the practice trials from the dataset (see the _trial.type_ variable)  
    
```{r}
df <- df %>% filter(trial.type == 'experiment')
```
    
    iii. Create a _correct_ variable  
    
```{r}
df$correct <- as.factor(ifelse(df$obj.resp == 'e' & df$target.type == 'even' | df$obj.resp == 'o' & df$target.type == 'odd', 1, 0))
```

    iv. Describe how the _target.contrast_ and _target.frames_ variables differ compared to the data from part 1 of this assignment  

```{r}
unique(df$target.contrast)
unique(df$target.frames)
```


The target contrast is always at 0.1.

# EXERCISE 5 - Use log-likelihood ratio tests to evaluate logistic regression models

1) Do logistic regression - _correct_ as the dependent variable and _target.frames_ as the independent variable. (Make sure that you understand what _target.frames_ encode). Create two models - a pooled model and a partial-pooling model. The partial-pooling model should include a subject-specific intercept.  

```{r}
cp_m <- glm(correct ~ target.frames, data = df, family = binomial) # complete pooling
pp_m <- glmer(correct ~ target.frames + (1 | subject), data = df, family = binomial) # partial-pooling
```

    i. the likelihood-function for logistic regression is: $L(p)={\displaystyle\prod_{i=1}^Np^{y_i}(1-p)^{(1-y_i)}}$ (Remember the probability mass function for the Bernoulli Distribution). Create a function that calculates the likelihood.  
    
```{r}
# likelihood-function for logistic regression = Binomial Probability

df$correct_numeric <- as.numeric(df$correct)-1

lik_fun <- function(y, y_hat) {
  vector <- 0
  for (i in 1:length(y)) {
    vector[i]<- ((y_hat[i])**(y[i])) * (1-(y_hat[i]))**(1-(y[i]))
  }
  prod(vector)
}

lik_fun(df$correct_numeric, boot::inv.logit(fitted.values(cp_m)))

```
    
    ii. the log-likelihood-function for logistic regression is: $l(p) = {\displaystyle\sum_{i=1}^N}[y_i\ln{p}+(1-y_i)\ln{(1-p)}$. Create a function that calculates the log-likelihood 
    
```{r}
loglik_fun <- function(y, y_hat) {
  y*log(y_hat)+(1-y)*log(1-y_hat)
}
```

    iii. apply both functions to the pooling model you just created. Make sure that the log-likelihood matches what is returned from the _logLik_ function for the pooled model. Does the likelihood-function return a value that is surprising? Why is the log-likelihood preferable when working with computers with limited precision?  
    
```{r}
sum(loglik_fun(df$correct_numeric, cp_m$fitted.values))
logLik(cp_m)
# these values match

lik_fun(df$correct_numeric, cp_m$fitted.values)
# returns zero

```

The likelihood-function squares values that are close to zero, which quickly results in values that are rounded to zero on computers with limited precision.

    iv. now show that the log-likelihood is a little off when applied to the partial pooling model - (the likelihood function is different for the multilevel function - see section 2.1 of https://www.researchgate.net/profile/Douglas-Bates/publication/2753537_Computational_Methods_for_Multilevel_Modelling/links/00b4953b4108d73427000000/Computational-Methods-for-Multilevel-Modelling.pdf if you are interested)  
    
```{r}
sum(loglik_fun(df$correct_numeric, fitted.values(pp_m)))

logLik(pp_m)
```
    
    
2) Use log-likelihood ratio tests to argue for the addition of predictor variables, start from the null model, `glm(correct ~ 1, 'binomial', data)`, then add subject-level intercepts, then add a group-level effect of _target.frames_ and finally add subject-level slopes for _target.frames_. Also assess whether or not a correlation between the subject-level slopes and the subject-level intercepts should be included.

```{r}
m1 <- glm(correct~1, family='binomial', data=df)

m2 <- glmer(correct~1 + (1|subject), family='binomial', data=df)

m3 <- glmer(correct~1 + target.frames + (1|subject), family='binomial', data=df) # 'group level fx' = 'fixed fx'

m4 <- glmer(correct~1 + target.frames + (1+target.frames|subject), family='binomial', data=df)


lol <- ranef(m4)

for (i in seq(lol)){
  assign(paste0("lol", i), lol[[i]])
}

colnames(lol1) <- c("Intercept", "target.frames")

#We now plot the correlation between the random effects of intercept and slope.
plot(lol1$Intercept, lol1$target.frames, ylab="target.frames", xlab="intercept", main="Correlation between random effects of model 4")

VarCorr(m4)
```

From the plot we can observe that there is a negative correlation between the intercept and target.frames. We can also observe this from the VarCorr of model 4, finding a value of -0.87. Subjects that are above the global mean intercept, have less steep slopes. Essentially this means that subjects who have a higher baseline accuracy benefit less from additional frames than subjects with lower baseline accuracy.

    i. write a short methods section and a results section where you indicate which model you chose and the statistics relevant for that choice. Include a plot of the estimated group-level function with `xlim=c(0, 8)` that includes the estimated subject-specific functions.
    
```{r}
anova(m4, m3, m2, m1)

summary(m4)
```

The increasingly complex models are significantly better than their less complex counterparts, and model 4 ('m4') is the best one, as it also has the lowest AIC and BIC score. Model 4 is significantly different from model 3 (χ²(2)=346.41, p<0.001).

The beta coefficient (target.frames) is statistically significant from the null hypothesis (B = 0.83316, SE = 0.04432, p<0.05). In other words, the probability of giving a correct answer increases by 70% for each additional target frame.
Model4: Fixed effects intercept is -1.09 
    
```{r}
#changed boot::inv.logit(fitted(m4)) to just fitted(m4), seems to make more sense

df2 <- cbind(df, fitted = fitted(m4), probabilities = (fitted(m4)))
ggplot(df2, aes(x = target.frames, y = probabilities, color = subject)) + 
  geom_line() +
  xlim(0, 8) +
  labs(x = "Number of target frames",
       y = "Probability of correct answer",
       title = "Estimated function by subject") +
  scale_color_discrete(name = "Subject")
```
    
    
    ii. also include in the results section whether the fit didn't look good for any of the subjects. If so, identify those subjects in the report, and judge (no statistical test) whether their performance (accuracy) differed from that of the other subjects. Was their performance better than chance? (Use a statistical test this time) (50 %)
    
The plot shows that the fit of subject 24 is off and the subject has lower accuracy than the rest.

```{r}
# We are unsure whether to test if their real performance or modeled performance is better than chance.

df_test <- data.frame(df, fitted(m4))
df_24 <- df_test %>% filter(subject == '24')
t.test(df_24$fitted.m4., mu=0.5)
```


3) Now add _pas_ to the group-level effects - if a log-likelihood ratio test justifies this, also add the interaction between _pas_ and _target.frames_ and check whether a log-likelihood ratio test justifies this  

```{r}
library(dfoptim)

m5 <- glmer(correct~1 + target.frames + target.frames + pas + (1+target.frames|subject), family='binomial', data=df)
summary(m5)

m6 <- glmer(correct~1 + target.frames + pas*target.frames + (1+target.frames|subject), family='binomial', data=df, control=glmerControl(optimizer='bobyqa'))
summary(m6)


anova(m6, m5, m4)
```


    i. if your model doesn't converge, try a different optimizer  
    
    ii. plot the estimated group-level functions over `xlim=c(0, 8)` for each of the four PAS-ratings - add this plot to your report (see: 5.2.i) and add a description of your chosen model. Describe how _pas_ affects accuracy together with target duration if at all. Also comment on the estimated functions' behaviour at target.frame=0 - is that behaviour reasonable?  

```{r}
df_m6 <- data.frame(df, fitted(m6))

five_3_ii <- ggplot(df_m6, aes(x = target.frames, y = fitted.m6., color = subject)) + 
  geom_line() +
  xlim(0, 8) +
  facet_wrap(~pas)+
  labs(x = "Number of target frames",
       y = "Probability of correct answer",
       title = "Estimated function by subject and PAS-rating") +
  scale_color_discrete(name = "Subject")

five_3_ii
```

When subjects' PAS-ratings are 1, the number of target frames does not have any systematic effect on accuracy. For PAS-ratings of 2, there is a near-linear (if not slightly logarithmic) growth curve between the number of target frames and accuracy. For PAS-ratings of 3 and 4, we observe a logarithmic growth curve, meaning that the first additional frames increase accuracy more than the last additional frames.

At target.frame=0 and PAS=1, the probability of a correct answer is 47%. This is close enough to chance (i.e. 50%) that we can conclude it is reasonable, because if no frames are shown, the participant can only guess. The achieved value of 47% is a lucky coincidence though, resulting from the fact that the slope for PAS-1 is nearly horizontal.

At target.frame=0 and PAS-ratings above 1 the model clearly falls apart (PAS-2 = 33%, PAS-3 = 34%, PAS-4 = 52%), because it doesn't account for the fact that the lowest possible probability of a correct answer should be 50% in this sort of forced-choice experiment with two options.

# EXERCISE 6 - Test linear hypotheses

In this section we are going to test different hypotheses. We assume that we have already proved that more objective evidence (longer duration of stimuli) is sufficient to increase accuracy in and of itself and that more subjective evidence (higher PAS ratings) is also sufficient to increase accuracy in and of itself.  
We want to test a hypothesis for each of the three neighbouring differences in PAS, i.e. the difference between 2 and 1, the difference between 3 and 2 and the difference between 4 and 3. More specifically, we want to test the hypothesis that accuracy increases faster with objective evidence if subjective evidence is higher at the same time, i.e. we want to test for an interaction.  

1) Fit a model based on the following formula: `correct ~ pas * target.frames + (target.frames | subject))`
    i. First, use `summary` (yes, you are allowed to!) to argue that accuracy increases faster with objective evidence for PAS-2 than for PAS-1. 
    
```{r}
m7 <- glmer(correct ~ pas * target.frames + (target.frames|subject), family='binomial', data=df)
summary(m7)
```

We can observe that pas2:target.frames is 0.44718 (p<.05), which is a positive value, confirming that accuracy increases faster with each additional frame for PAS-2 vs PAS-1.

2) `summary` won't allow you to test whether accuracy increases faster with objective evidence for PAS 3 than for PAS 2 (unless you use `relevel`, which you are not allowed to in this exercise). Instead, we'll be using the function `glht` from the `multcomp` package

    i. To redo the test in 6.1.i, you can create a _contrast_ vector. This vector will have the length of the number of estimated group-level effects and any specific contrast you can think of can be specified using this. For redoing the test from 6.1.i, the code snippet below will do
    
```{r}
library(multcomp)

## testing whether PAS 2 is different from PAS 1
contrast.vector <- matrix(c(0, 0, 0, 0, 0, 1, 0, 0), nrow=1)
gh <- glht(m7, contrast.vector)
print(summary(gh))
## as another example, we could also test whether there is a difference in
## intercepts between PAS 2 and PAS 3
contrast.vector <- matrix(c(0, -1, 1, 0, 0, 0, 0, 0), nrow=1)
gh2 <- glht(m7, contrast.vector)
print(summary(gh2))
```

    ii. Now test the hypothesis that accuracy increases faster with objective evidence for PAS 3 than for PAS 2.
    
```{r}
## testing whether PAS 3 is different from PAS 2
contrast.vector <- matrix(c(0, 0, 0, 0, 0, -1, 1, 0), nrow=1) #the zeros stand for the rows in the fixed effects summary
gh <- glht(m7, contrast.vector)
print(summary(gh))
```

0.30151, p<.001

    iii. Also test the hypothesis that accuracy increases faster with objective evidence for PAS 4 than for PAS 3
    
```{r}
## testing whether PAS 4 is different from PAS 3
contrast.vector <- matrix(c(0, 0, 0, 0, 0, 0, -1, 1), nrow=1) #the zeros stand for the rows in the fixed effects summary
gh <- glht(m7, contrast.vector)
print(summary(gh))
```

0.0106, p>.05

3) Finally, test that whether the difference between PAS 2 and 1 (tested in 6.1.i) is greater than the difference between PAS 4 and 3 (tested in 6.2.iii)

We can observe that the difference between PAS-2 and 1 (0.44718) is larger than that of PAS-4 and 3 (0.0106).

# EXERCISE 7 - Estimate psychometric functions for the Perceptual Awareness Scale and evaluate them  

We saw in 5.3 that the estimated functions went below chance at a target duration of 0 frames (0 ms). This does not seem reasonable, so we will be trying a different approach for fitting here.  
We will fit the following function that results in a sigmoid, $f(x) = a + \frac {b - a} {1 + e^{\frac {c-x} {d}}}$  
It has four parameters: _a_, which can be interpreted as the minimum accuracy level, _b_, which can be interpreted as the maximum accuracy level, _c_, which can be interpreted as the so-called inflexion point, i.e. where the derivative of the sigmoid reaches its maximum and _d_, which can be interpreted as the steepness at the inflexion point. (When _d_ goes towards infinity, the slope goes towards a straight line, and when it goes towards 0, the slope goes towards a step function).  
  
We can define a function of a residual sum of squares as below

```{r}
RSS_func <- function(dataset, par)
{
    ## "dataset" should be a data.frame containing the variables x (target.frames)
    ## and y (correct)
    
    ## "par" are our four parameters (a numeric vector) 
    ## par[1]=a, par[2]=b, par[3]=c, par[4]=d
    x <- dataset$x
    y <- dataset$y
    y.hat <- par[1] + (par[2]-par[1]) / (1+exp(par[3]-x)/par[4]) ## sigmoid function
    RSS <- sum((y - y.hat)^2)
    return(RSS)
}
```

1) Now, we will fit the sigmoid for the four PAS ratings for Subject 7

```{r}
subject7 <- df %>% filter(subject == '7')
subject7$x <- subject7$target.frames
subject7$y <- subject7$correct_numeric

```

    i. use the function `optim`. It returns a list that among other things contains the four estimated parameters. You should set the following arguments:  
    `par`: you can set _c_ and _d_ as 1. Find good choices for _a_ and _b_ yourself (and argue why they are appropriate)  
    `fn`: which function to minimise?  
    `data`: the data frame with _x_, _target.frames_, and _y_, _correct_ in it  
    `method`: 'L-BFGS-B'  
    `lower`: lower bounds for the four parameters, (the lowest value they can take), you can set _c_ and _d_ as `-Inf`. Find good choices for _a_ and _b_ yourself (and argue why they are appropriate)  
    `upper`: upper bounds for the four parameters, (the highest value they can take) can set _c_ and _d_ as `Inf`. Find good choices for _a_ and _b_ yourself (and argue why they are appropriate)
    
```{r}
optim(par = c(0.5, 1, 1, 1),
      fn = RSS_func,
      data = subject7,
      method = 'L-BFGS-B',
      lower = c(0.5, 0.5, -Inf, -Inf),
      upper = c(1, 1, Inf, Inf)
      )

#Creating separate dataframes for each PAS-rating

df1 <- data.frame(subject = df$subject, x = df$target.frames, y = df$correct_numeric, pas = df$pas)

df_p1 <- df1 %>% filter(pas == '1')
df_p2 <- df1 %>% filter(pas == '2')
df_p3 <- df1 %>% filter(pas == '3')
df_p4 <- df1 %>% filter(pas == '4')

p1_o <- optim(par = c(0.5, 1, 1, 1),
      fn = RSS_func,
      data = df_p1,
      method = 'L-BFGS-B',
      lower = c(0.5, 0.5, -Inf, -Inf),
      upper = c(1, 1, Inf, Inf)
      )

p2_o <- optim(par = c(0.5, 1, 1, 1),
      fn = RSS_func,
      data = df_p2,
      method = 'L-BFGS-B',
      lower = c(0.5, 0.5, -Inf, -Inf),
      upper = c(1, 1, Inf, Inf)
      )

p3_o <- optim(par = c(0.5, 1, 1, 1),
      fn = RSS_func,
      data = df_p3,
      method = 'L-BFGS-B',
      lower = c(0.5, 0.5, -Inf, -Inf),
      upper = c(1, 1, Inf, Inf)
      )

p4_o <- optim(par = c(0.5, 1, 1, 1),
      fn = RSS_func,
      data = df_p4,
      method = 'L-BFGS-B',
      lower = c(0.5, 0.5, -Inf, -Inf),
      upper = c(1, 1, Inf, Inf)
      )

sigmoid_y_hat <- function(dataset, par)
{
    ## "dataset" should be a data.frame containing the variables x (target.frames)
    ## and y (correct)
    
    ## "par" are our four parameters (a numeric vector) 
    ## par[1]=a, par[2]=b, par[3]=c, par[4]=d
    x <- dataset$x
    y <- dataset$y
    y.hat <- par[1] + (par[2]-par[1]) / (1+exp(par[3]-x)/par[4]) ## sigmoid function
    return(y.hat)
}

df_p1$yhat <- sigmoid_y_hat(df_p1, p1_o$par)
df_p2$yhat <- sigmoid_y_hat(df_p2, p2_o$par)
df_p3$yhat <- sigmoid_y_hat(df_p3, p3_o$par)
df_p4$yhat <- sigmoid_y_hat(df_p4, p4_o$par)

subject7_p1 <- df_p1 %>% filter(subject == '7')
subject7_p2 <- df_p2 %>% filter(subject == '7')
subject7_p3 <- df_p3 %>% filter(subject == '7')
subject7_p4 <- df_p4 %>% filter(subject == '7')

subject7_p <- rbind(subject7_p1, subject7_p2, subject7_p3, subject7_p4)

```
    
    ii. Plot the fits for the PAS ratings on a single plot (for subject 7) `xlim=c(0, 8)`
    
```{r}
s7_p1 <- ggplot(subject7_p, aes(x = x, y = yhat, color = pas)) + 
  geom_line() +
  xlim(0, 8) +
  labs(x = "Number of target frames",
       y = "Probability of correct answer",
       title = "Subject 7") +
  scale_color_discrete(name = "PAS")
```
    
    iii. Create a similar plot for the PAS ratings on a single plot (for subject 7), but this time based on the model from 6.1 `xlim=c(0, 8)`   
    
```{r}
df_subject7 <- df
df_subject7$fitted_m7 <- fitted.values(m7)
df_subject7 <- df_subject7 %>%
  filter(subject== '7')

s7_p2 <- ggplot(df_subject7, aes(x = target.frames, y = fitted_m7, color = pas)) + 
  geom_line() +
  xlim(0, 8) +
  labs(x = "Number of target frames",
       y = "Probability of correct answer",
       title = "Subject7, model 7") +
  scale_color_discrete(name = "PAS")

library(gridExtra)
grid.arrange(s7_p1, s7_p2)
```
    
    iv. Comment on the differences between the fits - mention some advantages and disadvantages of each way  
    
The fit of model 7 assigns slightly lower probabilities across all PAS-ratings.

Model 7, unlike the other fit, does not take into account that 0.5 should be the
lowest possible probability, and assigns values less than 0.5 to PAS-ratings 1
and 2 at one target frame.

2) Finally, estimate the parameters for all subjects and each of their four PAS ratings. Then plot the estimated function at the group-level by taking the mean for each of the four parameters, _a_, _b_, _c_ and _d_ across subjects. A function should be estimated for each PAS-rating (it should look somewhat similar to Fig. 3 from the article:  https://doi.org/10.1016/j.concog.2019.03.007)


```{r}
n <- length(unique(df1$subject))

for (i in 1:n) {
  df_bla <- df1 %>% 
  filter(subject == i) %>% 
  dplyr::select(x, y, pas)
  
  pas1 <- optim(par = c(0.5,1,1,1), fn=RSS_func, data = filter(df_bla, pas == "1"), method = 'L-BFGS-B', lower = c(0.5,0.5,-Inf,-Inf), upper = c(1,1,Inf,Inf))
  pas2 <- optim(par = c(0.5,1,1,1), fn=RSS_func, data = filter(df_bla, pas == "2"), method = 'L-BFGS-B', lower = c(0.5,0.5,-Inf,-Inf), upper = c(1,1,Inf,Inf))
  pas3 <- optim(par = c(0.5,1,1,1), fn=RSS_func, data = filter(df_bla, pas == "3"), method = 'L-BFGS-B', lower = c(0.5,0.5,-Inf,-Inf), upper = c(1,1,Inf,Inf))
  pas4 <- optim(par = c(0.5,1,1,1), fn=RSS_func, data = filter(df_bla, pas == "4"), method = 'L-BFGS-B', lower = c(0.5,0.5,-Inf,-Inf), upper = c(1,1,Inf,Inf))
  
  df_bla$yhat_pas1_all <- pas1$par[1] + ((pas1$par[2]-pas1$par[1])/(1+exp(1)^((pas1$par[3]-df_bla$x)/pas1$par[4])))
  df_bla$yhat_pas2_all <- pas2$par[1] + ((pas2$par[2]-pas2$par[1])/(1+exp(1)^((pas2$par[3]-df_bla$x)/pas2$par[4])))
  df_bla$yhat_pas3_all <- pas3$par[1] + ((pas3$par[2]-pas3$par[1])/(1+exp(1)^((pas3$par[3]-df_bla$x)/pas3$par[4])))
  df_bla$yhat_pas4_all <- pas4$par[1] + ((pas4$par[2]-pas4$par[1])/(1+exp(1)^((pas4$par[3]-df_bla$x)/pas4$par[4])))


}

df_bla_x1 <- df_bla %>% 
  filter(x == 1) %>% 
  group_by(pas) %>% 
  summarise(mean(yhat_pas1_all), mean(yhat_pas2_all), mean(yhat_pas3_all), mean(yhat_pas4_all))
df_bla_x1$x = 1

df_bla_x2 <- df_bla %>% 
  filter(x == 2) %>% 
  group_by(pas) %>% 
  summarise(mean(yhat_pas1_all), mean(yhat_pas2_all), mean(yhat_pas3_all), mean(yhat_pas4_all))
df_bla_x2$x = 2

df_bla_x3 <- df_bla %>% 
  filter(x == 3) %>% 
  group_by(pas) %>% 
  summarise(mean(yhat_pas1_all), mean(yhat_pas2_all), mean(yhat_pas3_all), mean(yhat_pas4_all))
df_bla_x3$x = 3

df_bla_x4 <- df_bla %>% 
  filter(x == 4) %>% 
  group_by(pas) %>% 
  summarise(mean(yhat_pas1_all), mean(yhat_pas2_all), mean(yhat_pas3_all), mean(yhat_pas4_all))
df_bla_x4$x = 4

df_bla_x5 <- df_bla %>% 
  filter(x == 5) %>% 
  group_by(pas) %>% 
  summarise(mean(yhat_pas1_all), mean(yhat_pas2_all), mean(yhat_pas3_all), mean(yhat_pas4_all))
df_bla_x5$x = 5

df_bla_x6 <- df_bla %>% 
  filter(x == 6) %>% 
  group_by(pas) %>% 
  summarise(mean(yhat_pas1_all), mean(yhat_pas2_all), mean(yhat_pas3_all), mean(yhat_pas4_all))
df_bla_x6$x = 6

df_mean <- as.data.frame(rbind(df_bla_x1, df_bla_x2, df_bla_x3, df_bla_x4, df_bla_x5, df_bla_x6))

all_subjects_mean <- df_mean %>% 
  ggplot() + 
  geom_line(aes(x = x, y = `mean(yhat_pas1_all)`, color = "pas1")) + 
  geom_line(aes(x = x, y = `mean(yhat_pas2_all)`, color = "pas2")) + 
  geom_line(aes(x = x, y = `mean(yhat_pas3_all)`, color = "pas3")) + 
  geom_line(aes(x = x, y = `mean(yhat_pas4_all)`, color = "pas4")) +
  labs(title = "All subjects",
       x = "Number of target frames",
       y = "Probability of correct answer") +
  theme_bw() 
```

    i. compare with the figure you made in 5.3.ii and comment on the differences between the fits - mention some advantages and disadvantages of both.
    
```{r}
grid.arrange(five_3_ii, all_subjects_mean)
```

The largest difference can be seen for the PAS-2 curve, which is much steeper
from target frames 1-3 in the lower figure, and then reaching the max. value of 1.
PAS-3 and PAS-4 are also at higher values in the lower figure.
